name: GPU CI/CD Pipeline (with CPU Fallback)

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

env:
  ROS_DISTRO: humble
  CUDA_VERSION: "12.4.1"

jobs:
  # GPU環境チェック（セルフホスト）
  gpu-check:
    runs-on: [self-hosted, gpu]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Check GPU availability
        run: |
          echo "Checking GPU environment..."
          nvidia-smi
          echo "GPU available"
          
      - name: Check CUDA environment
        run: |
          python3 -c "
          import torch
          print(f'PyTorch version: {torch.__version__}')
          print(f'CUDA available: {torch.cuda.is_available()}')
          if torch.cuda.is_available():
              print(f'GPU count: {torch.cuda.device_count()}')
              print(f'GPU name: {torch.cuda.get_device_name(0)}')
          "

  # CPU環境チェック（GitHub Hosted）
  cpu-check:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install CPU dependencies
        run: |
          pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2
          pip install ray[rllib]==2.8.0
          pip install -r requirements-gpu.txt
          
      - name: Check CPU environment
        run: |
          python3 -c "
          import torch
          print(f'PyTorch version: {torch.__version__}')
          print(f'CUDA available: {torch.cuda.is_available()}')
          print('Running on CPU')
          "

  # GPU対応ビルド（セルフホスト）
  gpu-build:
    runs-on: [self-hosted, gpu]
    needs: gpu-check
    strategy:
      matrix:
        dockerfile:
          - "docker/Dockerfile.base-gpu"
          - "docker/Dockerfile.rl-agent-gpu"
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        
      - name: Build GPU Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ${{ matrix.dockerfile }}
          push: false
          tags: drone-rl-gpu:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max
          
      - name: Test GPU environment
        run: |
          docker run --rm --gpus all drone-rl-gpu:latest \
            python3 -c "
          import torch
          print(f'PyTorch version: {torch.__version__}')
          print(f'CUDA available: {torch.cuda.is_available()}')
          if torch.cuda.is_available():
              print(f'GPU count: {torch.cuda.device_count()}')
              print(f'GPU name: {torch.cuda.get_device_name(0)}')
          "

  # CPU対応ビルド（GitHub Hosted）
  cpu-build:
    runs-on: ubuntu-latest
    needs: cpu-check
    strategy:
      matrix:
        dockerfile:
          - "docker/Dockerfile.base"
          - "docker/Dockerfile.rl-agent"
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        
      - name: Build CPU Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ${{ matrix.dockerfile }}
          push: false
          tags: drone-rl-cpu:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max
          
      - name: Test CPU environment
        run: |
          docker run --rm drone-rl-cpu:latest \
            python3 -c "
          import torch
          print(f'PyTorch version: {torch.__version__}')
          print(f'CUDA available: {torch.cuda.is_available()}')
          print('Running on CPU')
          "

  # GPU対応テスト（セルフホスト）
  gpu-test:
    runs-on: [self-hosted, gpu]
    needs: gpu-build
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install GPU dependencies
        run: |
          pip install torch==2.1.2+cu121 torchvision==0.16.2+cu121 torchaudio==2.1.2+cu121 --index-url https://download.pytorch.org/whl/cu121
          pip install ray[rllib]==2.8.0
          pip install -r requirements-gpu.txt
          
      - name: Run GPU tests
        run: |
          python3 -c "
          import torch
          import ray
          from distributed_rl_env import DistributedDroneEnv
          
          # GPU環境テスト
          print('Testing GPU environment...')
          assert torch.cuda.is_available(), 'CUDA not available'
          print(f'GPU: {torch.cuda.get_device_name(0)}')
          
          # Ray初期化テスト
          ray.init(num_gpus=1, local_mode=True)
          print('Ray initialized successfully')
          
          # 分散環境テスト
          env = DistributedDroneEnv(num_envs=2, use_gpu=True)
          obs, info = env.reset()
          print(f'Environment initialized: {obs.shape}')
          env.close()
          
          print('All GPU tests passed!')
          "

  # CPU対応テスト（GitHub Hosted）
  cpu-test:
    runs-on: ubuntu-latest
    needs: cpu-build
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install CPU dependencies
        run: |
          pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2
          pip install ray[rllib]==2.8.0
          pip install -r requirements-gpu.txt
          
      - name: Run CPU tests
        run: |
          python3 -c "
          import torch
          import ray
          from distributed_rl_env import DistributedDroneEnv
          
          # CPU環境テスト
          print('Testing CPU environment...')
          print(f'PyTorch version: {torch.__version__}')
          print(f'CUDA available: {torch.cuda.is_available()}')
          
          # Ray初期化テスト
          ray.init(num_cpus=4, local_mode=True)
          print('Ray initialized successfully')
          
          # 分散環境テスト（CPU版）
          env = DistributedDroneEnv(num_envs=2, use_gpu=False)
          obs, info = env.reset()
          print(f'Environment initialized: {obs.shape}')
          env.close()
          
          print('All CPU tests passed!')
          "

  # 分散学習テスト（セルフホスト）
  distributed-test-gpu:
    runs-on: [self-hosted, gpu]
    needs: gpu-test
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          pip install torch==2.1.2+cu121 torchvision==0.16.2+cu121 torchaudio==2.1.2+cu121 --index-url https://download.pytorch.org/whl/cu121
          pip install ray[rllib]==2.8.0
          pip install -r requirements-gpu.txt
          
      - name: Test distributed training (GPU)
        run: |
          python3 src/high_performance_training.py \
            --config config/rl_config.yaml \
            --mode profile \
            --algorithm PPO \
            --duration 60

  # 分散学習テスト（GitHub Hosted）
  distributed-test-cpu:
    runs-on: ubuntu-latest
    needs: cpu-test
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2
          pip install ray[rllib]==2.8.0
          pip install -r requirements-gpu.txt
          
      - name: Test distributed training (CPU)
        run: |
          python3 src/high_performance_training.py \
            --config config/rl_config.yaml \
            --mode profile \
            --algorithm PPO \
            --duration 30

  # パフォーマンスベンチマーク（セルフホスト）
  performance-benchmark-gpu:
    runs-on: [self-hosted, gpu]
    needs: distributed-test-gpu
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          pip install torch==2.1.2+cu121 torchvision==0.16.2+cu121 torchaudio==2.1.2+cu121 --index-url https://download.pytorch.org/whl/cu121
          pip install ray[rllib]==2.8.0
          pip install -r requirements-gpu.txt
          
      - name: Run performance benchmarks (GPU)
        run: |
          python3 tools/gpu_monitor.py --duration 30 --export benchmark_results_gpu.json
          
      - name: Upload benchmark results (GPU)
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-gpu
          path: benchmark_results_gpu.json

  # パフォーマンスベンチマーク（GitHub Hosted）
  performance-benchmark-cpu:
    runs-on: ubuntu-latest
    needs: distributed-test-cpu
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2
          pip install ray[rllib]==2.8.0
          pip install -r requirements-gpu.txt
          
      - name: Run performance benchmarks (CPU)
        run: |
          python3 tools/gpu_monitor.py --duration 30 --export benchmark_results_cpu.json
          
      - name: Upload benchmark results (CPU)
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-cpu
          path: benchmark_results_cpu.json

  # セキュリティスキャン
  security-scan:
    runs-on: ubuntu-latest
    needs: [performance-benchmark-gpu, performance-benchmark-cpu]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Run security scan
        uses: snyk/actions/docker@master
        env:
          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
        with:
          image: drone-rl-gpu:latest
          args: --severity-threshold=high

  # デプロイメント
  deploy:
    runs-on: ubuntu-latest
    needs: [security-scan]
    if: github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Deploy to registry
        run: |
          echo "Deploying GPU and CPU images to registry..."
          # ここにデプロイメントロジックを追加
          
      - name: Update documentation
        run: |
          echo "Updating GPU and CPU documentation..."
          # ここにドキュメント更新ロジックを追加 